{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc07021",
   "metadata": {},
   "source": [
    "# Activation Functions:\n",
    "Activation functions are mathematical  functions applied to the outputs of individual neurons in a neural network. They introduce non-linearity into the the network, allowing it to learn and approximate complex relationships between inputs and outputs.\n",
    "\n",
    "Some commonly used activation functions in deep learning:\n",
    "    \n",
    "(1). Sigmoid function(Logistic function):It maps the input to a value between 0 and 1. It was widely used in the past but is now less popular due to some  drawbacks such as vanishing gradients.\n",
    "    \n",
    "(2). Hyperbolic tangent function(tanh): Similar to the sigmoid function ,but it maps the input to a value between -1 and 1.It is still used in some cases, but it also suffers from vanishing gradient.\n",
    "    \n",
    "(3). Rectified Linear Unit (ReLU): This function sets all negative values to zero and keeps positive values unchanged. It is the most popular activation function in deep learning due to its simplicity and effectiveness in training deep neural networks.\n",
    "    \n",
    "(4). Leaky ReLU: This function is similar to ReLu but allows a small negative input values.It helps mitigate the \"dyingReLU\" probelm where some neurons can become permanently inactive during training.\n",
    "    \n",
    "(5). Parametric ReLU(PReLU):PReLU is a generalization of ReLU that introduces a learnable parameter to determine the slope of negative input values.It offers more flexibility and can improve model performance.\n",
    "    \n",
    "(6). Exponential Linear Unit (ELU):ELU is a variation of ReLU that alloes a smooth exponential decay.It helps alleviate the dying ReLU problem and can produce more robust models.\n",
    "    \n",
    "(7). Softmax: Softmax is commonly used in the output layer of a neural network for multi-class classification problems.It normalizes the output values to represnt probablity, ensuring that the sum of all probablities is 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b41985",
   "metadata": {},
   "source": [
    "# Activation functions are essential in deep learning for the following reasons:\n",
    "\n",
    "(1). Non-Linearity: Activation functions introduce non-linear transformation to the network, enabling it to learn complex patterns and relationships in the data. Without acitvation functions ,a neural network would simply be a linear model.\n",
    "    \n",
    "(2). Gradient propogation: Activation functions help propogate gradients backward during the training process, allowing efficient optimization and learning.Different activation functions have different characterstics in terms of gradient behaiour.which can impact the model's training dynamics.\n",
    "    \n",
    "(3). Model capacity: The choice of activation function can influence the capacity and expressive power of a neural network. Non-Linear activation functions enable the network to represent more complex functions, expanding its ability to learn and generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91f6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcc37f16",
   "metadata": {},
   "source": [
    "# 1. Sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "307da0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid( 2.5 )= 0.9241418199787566\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#Example usage \n",
    "x=2.5\n",
    "result=sigmoid(x)\n",
    "print(\"Sigmoid(\",x,\")=\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ce879a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81fd95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Classification dataset\n",
    "x,y=make_classification(n_samples=1000,n_features=10,n_informative=5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4cfa0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing sets\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cccd0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the neural network model\n",
    "model=Sequential()\n",
    "model.add(Dense(64,activation='tanh',input_shape=(x_train.shape[1],)))#input layer\n",
    "model.add(Dense(64,activation='tanh'))#hiden layer\n",
    "model.add(Dense(1,activation='sigmoid'))#target layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98a1863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d17bcd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "25/25 [==============================] - 1s 1ms/step - loss: 0.5567 - accuracy: 0.7113\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.8200\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3991 - accuracy: 0.8175\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.3737 - accuracy: 0.8462\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3590 - accuracy: 0.8562\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 966us/step - loss: 0.3404 - accuracy: 0.8575\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.3256 - accuracy: 0.8625\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3054 - accuracy: 0.8725\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8763\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 975us/step - loss: 0.2788 - accuracy: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23b8efde850>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train,y_train,epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "068a30e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.9000\n",
      "Test Loss 0.24464663863182068\n",
      "Test Accuracy 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the testing data\n",
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(\"Test Loss\",loss)\n",
    "print(\"Test Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b6a3b",
   "metadata": {},
   "source": [
    "# 2. ReLU :\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d4de9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eae6c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the neural network architechture\n",
    "input_size=4\n",
    "hidden_size=8\n",
    "output_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64bb0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "model=keras.Sequential([\n",
    "    keras.layers.Dense(hidden_size,activation='relu',input_shape=(input_size,)),\n",
    "    keras.layers.Dense(output_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "228760c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "             loss=keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b6db6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your input and target data as numpy arrays\n",
    "input_data=np.array([[1.0,2.0,3.0,4.0],\n",
    "                   [2.0,3.0,4.0,5.0],\n",
    "                   [3.0,4.0,5.0,6.0]])\n",
    "target_data=np.array([[0.5,0.8],\n",
    "                    [0.6,0.9],\n",
    "                    [0.7,1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "531c515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.8322\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1297\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0175\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0028\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.1965e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5213e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1325e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0499e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0084e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9725e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23b9158ee90>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(input_data,target_data,epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0729e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "test_input=np.array([[1.0,2.0,3.0,4.0]])\n",
    "predicted_output=model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1806f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: [[0.32051203 0.75245047]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output:\",predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4f135",
   "metadata": {},
   "source": [
    "# 3. Leaky RLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2608371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2  2.  -0.1  3. ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_tensor=tf.constant([-1.0,2.0,-0.5,3.0])\n",
    "\n",
    "output_tensor=tf.nn.leaky_relu(input_tensor,alpha=0.2)\n",
    "\n",
    "print(output_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224a3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85cd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the neural network architecture\n",
    "input_size=4\n",
    "hidden_size=8\n",
    "output_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f801d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create the input and target tensors\n",
    "inputs=tf.keras.Input(shape=(input_size,))\n",
    "targets=tf.keras.Input(shape=(output_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3309294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the weights and biases for the hidden layer\n",
    "hidden_weights = tf.Variable(tf.random.normal(shape=(input_size,hidden_size)))\n",
    "hidden_biases=tf.Variable(tf.zeros(shape=(hidden_size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89edd03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(4, 8) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(8,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "#Compute nthe hidden layer output with laeky RelU activation function\n",
    "hidden_layer_output=tf.nn.leaky_relu(tf.matmul(inputs,hidden_weights)+hidden_biases,alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4643a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the weigths and biases for the Leaky ReLU activation funcction\n",
    "output_weights=tf.Variable(tf.random.normal(shape=(hidden_size,output_size)))\n",
    "output_biases=tf.Variable(tf.zeros(shape=(output_size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4219ee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(8, 2) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(2,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "#Compute the final output\n",
    "output=tf.matmul(hidden_layer_output,output_weights)+output_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9916408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss=tf.reduce_mean(tf.square(output-targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eecf921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d0b29b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create the model\n",
    "model=tf.keras.Model(inputs=[inputs,targets],outputs=output)\n",
    "model.add_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32431f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897b23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your input and target data as numpy arrays\n",
    "input_data=np.array([[1.0,2.0,3.0,4.0]])#replace with your input data\n",
    "target_data=np.array([[0.5,0.8]])#replace with your taregt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "514b175c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2251ab498d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit([input_data,target_data],epochs=1000,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d022ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the trained network\n",
    "test_input=np.array([[1.0,2.0,3.0,4.0]])\n",
    "test_target=np.array([[0.0,0.0]])#dummy target for prediction\n",
    "predicted_output=model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cdd7eb",
   "metadata": {},
   "source": [
    "# 4. Parametric ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfcbb95",
   "metadata": {},
   "source": [
    "# The Parametric Rectified Linear Unit activation function is an extension of the Leaky ReLU activation function that allows the slope of the negative part of the function to be learned during the training process.Instead of using a fixed slope value,PReLU intoduces a set of learnable parameters that control the slope.\n",
    "\n",
    "The PReLU function is defined as follows:\n",
    "\n",
    "PReLU(x)=max(0,x)+alpha*min(0.x)\n",
    "\n",
    "where \"x\" id=s the input value and 'alpha' is a learnable vector . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f506d",
   "metadata": {},
   "source": [
    "# 6. SoftMax Function\n",
    "\n",
    "The softmax function is primarily used in the output layer of a neural network for multi-class classification problems.It provides a convenient way to interpret the outputs as probabilities and make predictions based on the highest probability calss. The most common loss function used with softmax is the categorical cross-entropy loss,which measures the difference between the predicted probabilities and the true class labels.\n",
    "\n",
    "The softmax activation function is a commonly used activation function in the output layer of a layer of a neural network, particuraly for multi-class classification problems. It is designed to produce a probability distribution over mutliple classes, where the output values sum up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648802cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78edb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input data\n",
    "input_data=[[0.5,1.2,0.8],[2.1,0.3,1.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdf6969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a Sequential model\n",
    "model=tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35f5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add softmax activation function\n",
    "model.add(tf.keras.layers.Dense(3,input_shape=(3,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd9748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply softmax activation function\n",
    "model.add(tf.keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2f9e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3)                 12        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12 (48.00 Byte)\n",
      "Trainable params: 12 (48.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4514fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 201ms/step\n"
     ]
    }
   ],
   "source": [
    "#Compute softmax output\n",
    "softmax_output=model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3343b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output:\n",
      "[[0.6571169  0.19742642 0.14545669]\n",
      " [0.2208712  0.73685867 0.04227015]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Softmax Output:\")\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e231fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
