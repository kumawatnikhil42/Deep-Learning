{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4cecec",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network and its applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af1ed9e",
   "metadata": {},
   "source": [
    "They are type of neural network designed to process sequential data by maintining an internal state , or memory to capture information from previous inputs. They are particularly useful when dealing with sequential and temporal data as they can learn patterns and  dependencies over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc766d",
   "metadata": {},
   "source": [
    "Here are some key reasons why RNNs are used:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ad62e",
   "metadata": {},
   "source": [
    "1. Sequential Data Processing: RNN excel at processing sequrntial data where the order of elements matters such as time series data , natural language processing(NLP) speech recoginiton and handwritting recognition . They can capture dependices between different elements in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4cda61",
   "metadata": {},
   "source": [
    " 2. Variable Length Input : RNNs can handle variable length input and produce corresponding output of the same sequence length .This flexibility is valuable in applications where inputs or outputs have varying lengths such as text generation or speech synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb3290",
   "metadata": {},
   "source": [
    "3. Memory and Contextual Information: RNN maintain internal memory to store information about past inputs allowing them to reatin context and information from earlier elements in the sequence. This memory enables the network to make decision based on previous input and their relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05fca2",
   "metadata": {},
   "source": [
    " 4. Time Series Analysis : They are commonly used for analyzing time series data such as financial data , weather data , ad physiological signals . By considering the temporal nature of the data , RNN can model trends , dependencies and pattern over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91462082",
   "metadata": {},
   "source": [
    " 5. Natural Language Processing: RNNs have proven to  be highly effective in NLP tasks , including language , machine transaltion , setiment analysis , text classification and named entity recognition . They can capture the semantic and synthetic strucutre og language by processing words or character sequentially "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4118a7",
   "metadata": {},
   "source": [
    "# WHAT IS SEQUENTIAL DATA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de17237",
   "metadata": {},
   "source": [
    " It refers to a tpe of data where the order or sequence  of elements carries significance and affects the interpreatation or analysis of the data. In sequential data the position or arrangement of elements convelys information or patterns that need to be captured and understood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5b020",
   "metadata": {},
   "source": [
    " Real life sequential data include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91932e43",
   "metadata": {},
   "source": [
    "1. Time Series Data: I t is a classic example of sequential data It involves a sequence of data recorded over time where each point represent a measurmenet or observation taken at a specific time. Examples: INCLUDE STOCK PRICES , temperature recordings , heart rate monitoring and daily sales data . The order of the data point is crucial for understanding trends , seasonallity and patterns over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd337e3",
   "metadata": {},
   "source": [
    " 2. Natural Language Text: Textual data , such as sentences or pragarphs in natural language is ingerently sequentail . The order of words and characters carries meaning and determine the semantic and syntax of the text . language models , machine translation , sentiment analysis and text generation task all rely on capturing the sequential struture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64383ec",
   "metadata": {},
   "source": [
    " 3.Music and Audio Signals: They are sequential in nature . Musical notes played over time form a sequential that need to be captured to understand melodies , rhythms, and harmonies . Similarly audio signals like speech music recordings or envirnment sounds can be represented as a sequence of samples over time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e761ba4",
   "metadata": {},
   "source": [
    " 4. DNA Sequence : DNA sequence represent the order of nucleotides(adenine , thymine , sytosine , and guanine) that make up an organism's  genetic material .Analyzing and understanding DNA sequence is curical in various biological applications , including genetic research , disease diagnosis and evolutionary studeis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f609ef36",
   "metadata": {},
   "source": [
    "5. User behaviour Data: Sequential data can also be found in user behaviour logs such as web clickstrea or transaction histories. The order of actions taken by users provides insights into their browsing patterns , prefernces or purchasing behvaiour . Analyzing the sequential data can help optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad7a76e",
   "metadata": {},
   "source": [
    "# RNN Forward propagation with time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3037a",
   "metadata": {},
   "source": [
    "In an RNN , the input sequence is processed step by step over time and at each time step the network produces an output and updates its hidden state. The hidden state serves as the memory that retains information from previous time steps , allowing the network to capture dependencies and patterns over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ebbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the RNN parameters\n",
    "input_size = 4\n",
    "hidden_size = 3\n",
    "output_size = 2\n",
    "\n",
    "\n",
    "# Define the input sequence\n",
    "sequence= np.array([[1,2,3,4],\n",
    "                  [5,6,7,8],\n",
    "                  [9,10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e028ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the rnn weights\n",
    "\n",
    "Wxh = np.random.randn(hidden_size , input_size)\n",
    "Whh = np.random.randn(hidden_size , hidden_size)\n",
    "Why = np.random.randn(output_size , hidden_size)\n",
    "bh= np.zeros((hidden_size , 1)) \n",
    "by = np.zeros((output_size , 1)) #Bias output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd643b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.54943791 2.01388591]\n",
      "Output: [0.61093373 2.11002334]\n",
      "Output: [0.61550261 2.11695636]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the hidden state\n",
    "\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "\n",
    "# Perform forward propagation through time\n",
    "for x in sequence:\n",
    "    x = x.reshape(-1 , 1)\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev)+bh)\n",
    "    y = np.dot (Why , h)+by\n",
    "    print(\"Output:\", y.ravel())\n",
    "    \n",
    "    h_prev = h\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aea1f1",
   "metadata": {},
   "source": [
    "# RNN Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82edf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the RNN MODEL\n",
    "rnn= tf.keras.layers.SimpleRNN(units=64)\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Generate dummy data\n",
    "input_data = tf.random.normal(shape=(32,10,32))\n",
    "target_data = tf.random.normal(shape=(32,64))\n",
    "\n",
    "# Perform forward pass\n",
    "with tf.GradientTape()  as tape:\n",
    "    predictions = rnn(input_data)\n",
    "    loss_value = loss_fn(target_data , predictions)\n",
    "    \n",
    "    \n",
    "#Perform backward pass\n",
    "gradients = tape.gradient(loss_value , rnn.trainable_variables)\n",
    "\n",
    "# Update parameters\n",
    "optimizer.apply_gradients(zip(gradients , rnn.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0be535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
